{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Armijo-Goldstein conditions\n",
        "## April, 2024\n",
        "## A hands-on notebook by Fariman.AA and Kiani.M\n"
      ],
      "metadata": {
        "id": "8wBXuQUW3FHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-dimensional search methods play a critical role in solving multi-dimensional optimization problems. Iterative optimization algorithms, typically employ a line search at each iteration. Given a function $f(x)$ to be minimized, iterative algorithms for finding its minimum can be described as follows:\n",
        "\n",
        "$\n",
        "x^(k+1) = x^k + a_kd^k\n",
        "\\\\\n",
        "x^0 \\text{ is initial point and } a_k \\text{is chosen to minimize}\n",
        "\\\\\n",
        "Φ_k(a) = f(x^k+ad^k)\n",
        "\\\\\n",
        "d^k \\text{ is search direction and } a_k \\text{ is step size}\n",
        "$\n",
        "\n",
        "so choice of $a_k$ involves a one-dimensional minimization to ensure that:\n",
        "\n",
        "$\n",
        "f(x^(k+1)) < f(x^k)\n",
        "$"
      ],
      "metadata": {
        "id": "xbNfjTYH53iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Armijo condition\n",
        "In the realm of optimization algorithms, particularly iterative methods such as gradient descent, the Armijo condition serves as a crucial criterion for selecting an appropriate step size along a designated search direction. This condition guarantees two key properties:\n",
        "\n",
        "* Sufficient Descent: The step size must be sufficiently large to ensure a significant reduction in the function value compared to a basic linear approximation derived from the gradient.\n",
        "\n",
        "* Cautious Update: The step size is restricted from becoming excessively large, thereby preventing it from surpassing the minimum (or maximum) and potentially causing an increase in the function value.\n",
        "\n",
        "Let's delve into the mathematical underpinnings of the Armijo condition.\n",
        "\n",
        "$\n",
        "ɛ ∈ (0,1), γ > 1, η ∈ (ɛ,1)\n",
        "\\\\\n",
        "\\text{The Armijo condition ensures that } a_k\n",
        "\\text{ is not too large by requiring that: }\n",
        "\\\\\n",
        "Φ_k(a_k) <= Φ_k(0)+ɛa_kΦ'_k(0)\n",
        "\\\\\n",
        "\\text{and this ensures that } a_k \\text{ isnt too small} \\\\\n",
        "Φ_k(γa_k) >= Φ_k(0)+ɛγa_kΦ'_k(0)\n",
        "$\n",
        "\n",
        "Armijo condition is typically expressed as:\n",
        "\n",
        "$\n",
        "f(x+a*d) <= f(x) + b * a * ∇f(x)^t.d\n",
        "$\n",
        "\n",
        "* f(x) is the value of the objective function at the current point\n",
        "\n",
        "* a is the step size being considered.\n",
        "\n",
        "* d is the search direction.\n",
        "\n",
        "* b is a parameter typically chosen to be small, ensuring that the step size is sufficiently small.\n",
        "\n",
        "* $ \\nabla f(x) $ is the gradient of the objective function evaluated at $x$."
      ],
      "metadata": {
        "id": "2TVGTkQu7yjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Goldstein condition\n",
        "The Goldstein condition is a line search method used in optimization algorithms, particularly in gradient descent-based methods. It provides a condition for choosing an appropriate step size\n",
        "\n",
        "The Goldstein condition states that a step size t is acceptable if the following two conditions are satisfied:\n",
        "1.  \n",
        "$\n",
        "f(x_k+as_k) < f(x_k) + m_1a(s_k,g_k)\n",
        "$\n",
        "2.\n",
        "$\n",
        "f(x_k+as_k) > f(x_k) + m_2a(s_k,g_k)\n",
        "$\n",
        "* f: Objective function.\n",
        "* g: Gradient of the objective function.\n",
        "* x_k: Current search position.\n",
        "* s_k: Search direction.\n",
        "* a: Step size.\n",
        "* m1: Constant parameter for Goldstein condition (0 < m1 < 1).\n",
        "* m2: Constant parameter for Goldstein condition (m1 < m2 < 1).\n",
        "\n",
        "The Goldstein condition provides a balance between sufficient decrease in the objective function and avoiding excessively small step sizes."
      ],
      "metadata": {
        "id": "ibwOWnOn-4tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Armijo-Goldstein condition\n",
        "The first Armijo inequality with Goldstein condition are jointly called Armijo-Goldstein method. This method combines elements of both conditions to determine the step size during each iteration of the optimization process.\n"
      ],
      "metadata": {
        "id": "BJeFp7cCBtjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "dkoapvvQtWiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Armijo_Goldstein(f, grad_f, x, d, alpha, beta,m1,m2):\n",
        "    \"\"\"\n",
        "    Armijo condition for choosing the step size.\n",
        "\n",
        "    Parameters:\n",
        "        f (function): Objective function.\n",
        "        grad_f (function): Gradient of the objective function.\n",
        "        x (numpy.ndarray): Current point.\n",
        "        d (numpy.ndarray): Search direction.\n",
        "        alpha (float): Step size.\n",
        "        beta (float): Condition parameter.\n",
        "        m1,m2: Constant parameters for Goldstein conditions\n",
        "    Returns:\n",
        "        t: value that the satisties Armijo-Goldstein condition.\n",
        "    \"\"\"\n",
        "    max_iter=10000\n",
        "    t = alpha\n",
        "    for _ in range(max_iter):\n",
        "      # initial step size\n",
        "      # Armijo condition\n",
        "      armijoCondition = f(x + t * d) <= f(x) + beta * t * np.dot(grad_f(x), d)\n",
        "      # Goldstein conditions\n",
        "      g_k = grad_f(x)\n",
        "      lhs1 = f(x + t * d)\n",
        "      rhs1 = f(x) + m1 * t * np.dot(d, g_k)\n",
        "      rhs2 = f(x) + m2 * t * np.dot(d, g_k)\n",
        "\n",
        "      if lhs1 < rhs1 and lhs1 > rhs2 and armijoCondition:\n",
        "          return t\n",
        "      else:\n",
        "          t *= beta\n",
        "\n"
      ],
      "metadata": {
        "id": "D8gPt0erQpFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reviewing a simple example\n",
        "Having established the fundamental principles of the Armijo-Goldstein algorithm, let's now illustrate its application with an example. In this analysis, we will focus on the function $ f(x) = x^2 $. We will also determine its gradient, which plays a crucial role in the algorithm."
      ],
      "metadata": {
        "id": "ZK0fqw80XsqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define objective function and its gradient\n",
        "def f(x):\n",
        "    return x**2\n",
        "\n",
        "def grad_f(x):\n",
        "    return 2 * x\n"
      ],
      "metadata": {
        "id": "3azGsHrFZmje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can call Armijo_Goldstein() function to achive the best step size for this iteration."
      ],
      "metadata": {
        "id": "4kVUymtIZpnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current point\n",
        "x = 1.0\n",
        "\n",
        "# Search direction\n",
        "d = -grad_f(x)\n",
        "\n",
        "# Initial step size\n",
        "alpha = 1\n",
        "\n",
        "# Armijo condition parameter\n",
        "beta = 0.5\n",
        "# m1 : Constant parameter for Goldstein condition (0 < m1 < 1).\n",
        "# m2 : Constant parameter for Goldstein condition (m1 < m2 < 1).\n",
        "m1 = 0.1\n",
        "m2 = 0.9\n",
        "\n",
        "print(Armijo_Goldstein(f,grad_f,x,d,alpha,beta,m1,m2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0JBjDY9XrRG",
        "outputId": "291ceb8c-c189-42bc-b5a7-af292dbe741f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As demonstrated in the execution of the algorithm, the resulting step size was found to be 0.5.\n",
        "\n",
        "Good Luck!"
      ],
      "metadata": {
        "id": "394t_bovaJo_"
      }
    }
  ]
}